---
title: "arcticreport"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{arcticreport}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message = FALSE}
#library(arcticreport)
devtools::load_all()
library(EML)
library(dataone)
library(readr)
library(purrr)
library(DT)
library(dplyr)
library(tidyr)
library(jsonlite)
library(rt)
library(lubridate)
library(stringr)
```

```{r, message = FALSE}
Sys.setenv("RT_BASE_URL"="https://support.nceas.ucsb.edu/rt/")
rt_login()
quarters_file <- "../inst/extdata/quarters.csv"
quarters <- read_csv(quarters_file, progress = FALSE)
```

```{r}
# make sure you set a token!
# Cache tolerance is maximum number of days tolerable for age of cached results. If the cached results are older than the cache tolerance, a new dataset will be cached and the old cache deleted. To force a cache refresh, set tolerance to zero.
objs <- query_objects(cache_tolerance = 100000)
```


SSH into arcticdata.io, `cd /var/data/10.18739/`, and calculating directory size using: `du -csh --block-size=1K * | tr "\\t" "," > ~/filesize.csv`. This takes a while to run. Once it is finished, run `cat ~/filesize.csv` and copy the output into the block below, removing the "total" row.

```{r}

size_info <- tribble(
    ~ size_kb, ~ id,
1540,"A24Q7QR39",
8675840,"A2542J95X",
363518368,"A26688K9D",
93103396,"A26T0GX63",
4011719360,"A28911S0V",
1632101256,"A2CV4BS5K",
1364929160,"A2G73751P",
19814756,"A2G737525",
3500608,"A2H70820F",
1661364,"A2J38KJ9R",
687345560,"A2JQ0SW5Z",
2387058564,"A2KW57K4R",
3956508,"A2N29P78F",
30306888,"A2NC5SD71",
58359088,"A2QF8JK7T",
3615792,"A2QJ78024",
11787336,"A2R49GB1J",
103868388,"A2RV0D21Z",
18902208,"A2RV0D23X",
264,"A2S46H728",
1428,"A2SX64B5F",
572554340,"A2W08WH8M",
26485556,"A2ZG6G81T",
607299596,"A2ZK55N82",
)
```

For the next chunk, just run `stat -c "%.10y %n" * | tr " " ","` This one goes fast, it just gives the timestamps for each folder.

```{r}
date_info <- tribble(~date, ~id,
"2022-01-26","A24Q7QR39",
"2022-05-27","A2542J95X",
"2022-11-08","A26688K9D",
"2022-07-11","A26T0GX63",
"2021-10-13","A28911S0V",
"2021-02-11","A2CV4BS5K",
"2022-01-18","A2G73751P",
"2022-02-03","A2G737525",
"2022-01-11","A2H70820F",
"2022-10-12","A2J38KJ9R",
"2022-03-14","A2JQ0SW5Z",
"2022-07-28","A2KW57K4R",
"2023-01-04","A2N29P78F",
"2022-10-11","A2NC5SD71",
"2021-09-24","A2QF8JK7T",
"2022-02-03","A2QJ78024",
"2022-10-19","A2R49GB1J",
"2021-11-30","A2RV0D21Z",
"2022-12-21","A2RV0D23X",
"2022-10-11","A2S46H728",
"2022-05-26","A2SX64B5F",
"2022-03-15","A2W08WH8M",
"2021-09-03","A2ZG6G81T",
"2022-12-08","A2ZK55N82")


datasets_add <- left_join(size_info, date_info) %>% 
    mutate(size = size_kb*1024) %>% 
    mutate(dateUploaded = as.Date(date)) %>% 
    mutate(formatType = "DATA") %>% 
    select(-size_kb,-date)

objs <- bind_rows(objs, datasets_add)
```

This updates the list of tickets, and individual files with annual ticket information (stored in inst/extdata/). Previous years ticket files are stored there, and the current year's needs to be updated every quarter.

```{r, echo=FALSE, warning=FALSE}
update_ticket_list()
df <- update_annual_tix(2023)
```

Now, calculate all of the actual metrics!

```{r}
quarters$new_datasets <- map2_chr(quarters$from, quarters$to, .f = count_new_datasets, objects = objs)
quarters$new_changed_datasets <- map2_chr(quarters$from, quarters$to, .f = count_new_and_changed_datasets, objects = objs)
quarters$new_objects <- map2_chr(quarters$from, quarters$to, .f = count_data_objects, objects = objs)
quarters$volume <- map2_chr(quarters$from, quarters$to, .f = count_volume, objects = objs)
quarters$unique_creators <- map2_chr(quarters$from, quarters$to, .f = count_creators, objects = objs)
quarters$downloads <- map2_chr(quarters$from, quarters$to, .f = count_downloads)
quarters$citations <- map2_chr(quarters$from, quarters$to, .f = count_citations)
quarters$support_interactions <- map2_chr(quarters$from, quarters$to, .f = count_support_interactions)
```

```{r}
datatable(quarters)
```

## Unique Accessors

A special metric, this one is obtained from the metacat postgres. For now, update the dates in the query below and sent it to someone with access to the production db to run. They will drop the file on the server for you to read from.

```
COPY (
    SELECT * FROM access_log WHERE 
    date_logged > '2022-11-01 00:00' AND 
    date_logged < '2023-01-31 23:59' AND 
    lower(event) = 'read' 
   ORDER BY date_logged ASC
) 
TO '/tmp/access_log.csv' WITH CSV HEADER;
```

```{r}
count_unique_accessors("~/arcticreport/access_log.csv", from, to)
```

# Plots

```{r}
qs <- "2022-11-01"
qe <- "2023-01-31"
```


```{r}
plot_cumulative_metric(objs, type = "metadata", metric = "count")  +
    annotate("rect",
             xmin = as.Date(qs),
             xmax = as.Date(qe),
             ymin = 2500,
             ymax = 7000,
             fill = "gray",
             alpha = 0.4)+
    xlim(c(as.Date("2016-03-01"), as.Date(qe)))

ggsave("~/datasets.png", height = 4, width = 5)
```

```{r}
plot_cumulative_metric(objs, type = "data", metric = "count")  +
    annotate("rect",
             xmin = as.Date(qs),
             xmax = as.Date(qe),
             ymin = 450000,
             ymax = 1000000,
             fill = "gray",
             alpha = 0.4)+
    xlim(c(as.Date("2016-03-01"), as.Date(qe)))

ggsave("~/objs.png", height = 4, width = 5)
```


```{r}
plot_cumulative_metric(objs, type = "data", metric = "size") +
    annotate("rect",
             xmin = as.Date(qs),
             xmax = as.Date(qe),
             ymin = 0,
             ymax = 77,
             fill = "gray",
             alpha = 0.4)+
    xlim(c(as.Date("2016-03-01"), as.Date(qe)))

ggsave("~/size.png", height = 4, width = 5)
```




# NSF Programs

Another special metric, this one takes a while to run.

```{r}
# set up time period
from <- quarters$from[8]
to <- quarters$to[8]
# get the latest version (helps us more accurately read the NSF award numbers)
get_latest_version <- function(mn, pid){
    ids <- get_all_versions(mn, pid)
    return(ids[length(ids)])
}
```



```{r}
mn <- getMNode(CNode("PROD"), "urn:node:ARCTIC")

# filter down the list of metadata docs during the time period
m_q <- objs %>% 
    filter(formatType == "METADATA") %>% 
    filter(!grepl("*.dataone.org/portals|*.dataone.org/collections", formatId)) %>%
    filter(is.na(obsoletes)) %>%
    filter(dateUploaded >= from & dateUploaded <= to)

# get the most recent version (early versions might not have a valid funding number)
m_q$latest <- lapply(m_q$id, get_latest_version, mn = mn)
m_q$latest <- unlist(m_q$latest)
# extract award numbers
res <- c()
for (i in seq_along(m_q$latest)){
    
    doc <- read_eml(getObject(mn, m_q$latest[i]))
    if (!is.null(doc$dataset$project)){
        m_q$funding[i] <- paste(arcticdatautils::eml_get_simple(doc, "awardNumber"), collapse = ";")
    }
        else {
             m_q$funding[i] <- NA
        }
    
}
# clean up awards
funding <- m_q %>% 
    select(id, dateUploaded, funding) %>% 
    separate(funding, paste("funding", 1:5, sep="_"), sep=";", extra="drop") %>% 
    pivot_longer(cols = starts_with("funding"), names_to = "h", values_to = "funding") %>% 
    select(-h) %>% 
    filter(!is.na(funding) & funding != "") %>% 
    filter(nchar(funding) == 7)
# extract program names
for (i in 1:nrow(funding)){
    url <- paste0("https://api.nsf.gov/services/v1/awards.json?id=",funding$funding[i],"&printFields=fundProgramName")

    t <- fromJSON(url)
    if (!is.null(t$response$award$fundProgramName)){
        funding$programName[i] <- t$response$award$fundProgramName
    }
    else {funding$programName[i] <- "unknown"}
}    
```

```{r}
q <- funding %>% 
    group_by(programName) %>% 
    summarise(n = n())

DT::datatable(q, rownames = F)
```



## Disciplines

Another special metric.

```{r}
res <- list()
for (i in 50:nrow(m_q)){
    q <- dataone::query(mn, list(q = paste0('id:"', m_q$latest[i], '"'),
                                          fl = 'id,sem_annotation',
                                          sort = 'dateUploaded+desc',
                                          rows = 1000),
                                 as = "data.frame") 
    
    if (nrow(q) > 0){
        q <- q %>% 
            rename(latest = id)
    } else{
        q <- data.frame(id = m_q$vers[i], sem_annotation = NA)
    }
        
    
    res[[i]] <- left_join(q, m_q[i, ])
    
}

res <- do.call(bind_rows, res) 

adc_disc <- read.csv("https://raw.githubusercontent.com/NCEAS/adc-disciplines/main/adc-disciplines.csv") %>% 
    mutate(an_uri = paste0("https://purl.dataone.org/odo/ADCAD_", stringr::str_pad(id, 5, "left", pad = "0")))

res$category <- map(res$sem_annotation, function(x){
    t <- grep("*ADCAD*", x, value = TRUE)
    cats <- c()
    for (i in 1:length(t)){
        z <- which(adc_disc$an_uri == t[i])
        cats[i] <- adc_disc$discipline[z]
        
    }
    return(cats)
})

res_summ <- res %>% 
    unnest_wider(category, names_sep = "") %>% 
    select(-sem_annotation) %>% 
    pivot_longer(cols = starts_with("category"), names_to = "cat", values_to = "disc") %>% 
    filter(!is.na(disc)) %>% 
    group_by(disc) %>% 
    summarise(n = n())


res1 <- res_summ %>% 
    arrange(disc)

```


## RT Plot

This plot only goes on the one pager summary.

```{r}
# generate plot
tickets_result <- rt_ticket_search("Queue='arcticdata'",
                         orderby = "+Created",
                         format = "l",
                         fields = "id,Created,Resolved,LastUpdated,Status")
tickets <- tickets_result # Copy so we don't have to re-run query when debugging
tickets$Status <- ordered(tickets$Status, c("rejected", "new", "open", "stalled", "resolved"))

# Make all datetime fields actual datetimes
parse_rt_datetime_pst <- function(x) {
  lubridate::parse_date_time(x, 
                  orders = c("a b d H:M:S Y", # RT default
                             "Y-m-d H:M:S"),       # My customized form
                  tz = "America/Los_Angeles")
}

tickets <- tickets %>% 
  mutate(Created = parse_rt_datetime_pst(Created),
         Resolved = parse_rt_datetime_pst(Resolved),
         LastUpdated = parse_rt_datetime_pst(LastUpdated)) %>% 
  mutate(id = str_replace(id, "ticket/", "")) %>% 
  mutate(DaysOpen = round(as.numeric(now() - Created, units = "days")),
         DaysSinceLastUpdated = round(as.numeric(now() - LastUpdated, units = "days")))

# Add in friendlier datetime fields mirroring the normal ones
nice_format <- "%Y/%m/%d %H:%M"

tickets <- tickets %>% 
  mutate(Created_nice = format(Created, nice_format),
         Resolved_nice = format(Resolved, nice_format),
         LastUpdated_nice = format(LastUpdated, nice_format))


tot <- tickets %>% 
  select(id, Created, Resolved) %>% 
  gather(status, datetime, -id, na.rm = TRUE)

names(tot) <- c("id", "status", "datetime")

tot <- tot %>%
  group_by(status) %>% 
  arrange(datetime) %>% 
  mutate(count = 1, ccount = cumsum(count)) %>% 
  mutate(date = date(datetime))


ggplot(tot, aes(datetime, ccount, color = status)) + 
  geom_step() +
  labs(title = "Cumulative Tickets Created & Resolved Over Time", x = "Date", y = "Number of Tickets") +
  annotate("rect",
           xmin = ymd_hms("2022-08-01 00:00:00"),
           xmax = ymd_hms("2022-10-31 00:00:00"),
           ymin = 0,
           ymax = max(tot$ccount),
           fill = "gray",
           alpha = 0.4) +
  xlim(c(ymd_hms("2016-03-01 00:00:00"), ymd_hms("2022-10-31 00:00:00"))) +
  theme_bw() +
  theme(legend.position = "bottom")

ggsave("~/tix_q2.png", height = 4, width = 5)
```



